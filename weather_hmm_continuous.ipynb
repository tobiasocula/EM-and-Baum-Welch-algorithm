{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0b59fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "def forward(loga, logb, T, logpi, observations):\n",
    "    logalpha = np.empty((T, 4))\n",
    "    for i in range(4):\n",
    "        logalpha[0, i] = logpi[i] + logb[0, i]\n",
    "    for t in range(1, T):\n",
    "        for j in range(4):\n",
    "            logterms = [loga[i, j] + logalpha[t - 1, i] for i in range(4)]\n",
    "            logalpha[t, j] = np.logaddexp.reduce(logterms) + logb[t, j]\n",
    "        \n",
    "    return logalpha\n",
    "\n",
    "def backward(loga, logb, T):\n",
    "    logbeta = np.empty((T, 4))\n",
    "    logbeta[T - 1, :] = 0\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        for i in range(4):\n",
    "            logterms = [loga[i, j] + logb[t + 1, i] + logbeta[t + 1, j] for j in range(4)]\n",
    "            logbeta[t, i] = np.logaddexp.reduce(logterms)\n",
    "            \n",
    "    return logbeta\n",
    "\n",
    "def compute_gamma(logalpha, logbeta, T):\n",
    "    loggamma = np.empty((T, 4))\n",
    "    for t in range(T):\n",
    "        for i in range(4):\n",
    "            loggamma[t, i] = logalpha[t, i] + logbeta[t, i] - np.logaddexp.reduce([\n",
    "                logalpha[t, j] + logbeta[t, j] for j in range(4)\n",
    "            ])\n",
    "\n",
    "    return loggamma\n",
    "\n",
    "def compute_xi(logalpha, logbeta, loga, logb, observations, T):\n",
    "    xi = np.empty((T, 4, 4))\n",
    "    for t in range(T - 1):\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                logterms = []\n",
    "                for k in range(4):\n",
    "                    for l in range(4):\n",
    "                        logterms.append(\n",
    "                            logalpha[t, k] + loga[k, l] + logb[t + 1, l] + logbeta[t + 1, l]\n",
    "                        )\n",
    "                xi[t, i, j] = (\n",
    "                    logalpha[t, i] + loga[i, j] + logb[t + 1, j] + logbeta[t + 1, j]\n",
    "                    - np.logaddexp.reduce(logterms)\n",
    "                )\n",
    "    return xi\n",
    "\n",
    "def compute_a(loggamma, logxi, T):\n",
    "    loga = np.empty((4, 4))\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            loga[i, j] = (\n",
    "                np.logaddexp.reduce([logxi[t, i, j] for t in range(T - 1)])\n",
    "                - np.logaddexp.reduce([loggamma[t, i] for t in range(T - 1)])\n",
    "            )\n",
    "    return loga\n",
    "\n",
    "\n",
    "def compute_b(mus, Sigmas, dataset, T):\n",
    "    \"\"\"\n",
    "    mus: M x N mean returns (over time) per regime and asset\n",
    "    Sigmas: M x N x N the covariance matrix per regime\n",
    "    dataset: T x N matrix of returns\n",
    "\n",
    "    returns: logB, T x M, where logB[t,k] = log p(r_t | s_t = k)\n",
    "    \"\"\"\n",
    "\n",
    "    logb = np.empty((T, 4))\n",
    "    for t in range(T):\n",
    "        for k in range(4):\n",
    "            # if np.isnan(dataset[t, :]).any() or np.isnan(mus[k, :]).any() or np.isnan(Sigmas[k, :, :]).any():\n",
    "            #     raise AssertionError()\n",
    "            logb[t, k] = multivariate_normal.logpdf(dataset[t, :], mus[k, :], Sigmas[k, :, :])\n",
    "    return logb\n",
    "\n",
    "\n",
    "def compute_mus_sigmas(loggamma, dataset, T, eps=1e-04):\n",
    "    mus = np.empty((4, 2))\n",
    "    Sigmas = np.empty((4, 2, 2))\n",
    "    for m in range(4):\n",
    "        exp_gammas = np.empty(T)\n",
    "\n",
    "        # compute mus\n",
    "        exp_gammas = np.exp(loggamma[:, m])\n",
    "        den = exp_gammas.sum()\n",
    "        if den < eps:\n",
    "            # state got no responsibility -> reinitialize it\n",
    "            mus[m, :] = np.mean(dataset, axis=0) + np.random.normal(0, 0.01, size=2)\n",
    "            Sigmas[m, :, :] = np.cov(dataset.T) + eps * np.eye(2)\n",
    "            continue\n",
    "        \n",
    "        mus[m, :] = np.sum(exp_gammas[:, None] * dataset, axis=0) / den\n",
    "\n",
    "        # compute Sigmas\n",
    "        diffs = dataset - mus[m, :]\n",
    "        Sigmas[m, :, :] = (diffs.T @ (diffs * exp_gammas[:, None])) / den\n",
    "\n",
    "        # stabilize\n",
    "        Sigmas[m, :, :] = (Sigmas[m, :, :] + Sigmas[m, :, :].T) / 2\n",
    "        Sigmas[m, :, :] += eps * np.eye(2)\n",
    "\n",
    "    return mus, Sigmas\n",
    "\n",
    "\n",
    "def viterbi(logpi, logb, loga, observations, T):\n",
    "    logdelta = np.zeros((T, 4))\n",
    "    psi = np.zeros((T, 4))\n",
    "    # logdelta[t, j] = log probability of the best path ending in state j at time t\n",
    "    # psi[t, j] = state that maximizes prob of having been there, over all seq\n",
    "    # ended in j at time t\n",
    "    logdelta[0, :] = logpi + logb[:, observations[0]]\n",
    "    for t in range(1, T):\n",
    "        for j in range(4):\n",
    "            seq_probs = logdelta[t - 1, :] + loga[:, j]\n",
    "            logdelta[t, j] = np.max(seq_probs) + logb[j, observations[t]]\n",
    "            psi[t, j] = np.argmax(seq_probs)\n",
    "\n",
    "    states = np.zeros(T, dtype=int) # holds most likely states for each time\n",
    "    states[-1] = np.argmax(logdelta[-1, :])\n",
    "    p = np.max(logdelta[-1, :]) # highest probability over all states (ended in T)\n",
    "\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        states[t] = psi[t + 1, states[t + 1]]\n",
    "        # S_{T-1}=argmax over all j of (logdelta[T - 1, j])\n",
    "\n",
    "    return states, p\n",
    "\n",
    "def match_states_by_B(B_est, B_true):\n",
    "    # cost = L2 distance between emission distributions (rows)\n",
    "    cost = cdist(B_est, B_true, metric='euclidean')  # shape (N_est, N_true)\n",
    "    row_ind, col_ind = linear_sum_assignment(cost)\n",
    "    # row_ind[i] -> col_ind[i], we want a permutation array perm where perm[est_index] = true_index\n",
    "    perm = np.empty(B_est.shape[0], dtype=int)\n",
    "    perm[row_ind] = col_ind\n",
    "    return perm\n",
    "\n",
    "def permute_model(pi, A, B, perm):\n",
    "    # perm maps estimated-index -> true-index\n",
    "    # We return model reordered so index i now corresponds to true index perm[i].\n",
    "    # To compare, we need inverse perm that gives mapping: new_index -> old_index\n",
    "    # Simpler: build arrays aligned to true indices\n",
    "    N = len(perm)\n",
    "    pi_reordered = np.zeros_like(pi)\n",
    "    A_reordered = np.zeros_like(A)\n",
    "    B_reordered = np.zeros_like(B)\n",
    "    for est_i, true_i in enumerate(perm):\n",
    "        pi_reordered[true_i] = pi[est_i]\n",
    "    for est_i, true_i in enumerate(perm):\n",
    "        for est_j, true_j in enumerate(perm):\n",
    "            A_reordered[true_i, true_j] = A[est_i, est_j]\n",
    "    for est_i, true_i in enumerate(perm):\n",
    "        B_reordered[true_i] = B[est_i]\n",
    "    return pi_reordered, A_reordered, B_reordered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae52450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m states_total[\u001b[32m0\u001b[39m] = \u001b[38;5;28mint\u001b[39m(np.random.choice(\u001b[32m4\u001b[39m))\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(states_total[\u001b[32m0\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m observations_total[\u001b[32m0\u001b[39m, :] = np.random.multivariate_normal(\u001b[43mmus_real\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstates_total\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m, Sigma_real[states_total[\u001b[32m0\u001b[39m], :, :])\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, T + T_test):\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# sample next state according to transition probabilities from A_real\u001b[39;00m\n\u001b[32m     50\u001b[39m     states_total[t] = \u001b[38;5;28mint\u001b[39m(np.random.choice(\u001b[32m4\u001b[39m, p=A_real[states_total[t-\u001b[32m1\u001b[39m], :]))\n",
      "\u001b[31mIndexError\u001b[39m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T = 200\n",
    "T_test = 100\n",
    "\n",
    "A_std_noise = 0.05\n",
    "pi_std_noise = 0.05\n",
    "mus_std_noise = [2.0, 0.5] # temp, rain level\n",
    "Sigmas_std_noise = 0.02\n",
    "\n",
    "# columns and rows indexed by\n",
    "# (warm & rain, warm & dry, cold & rain, cold & dry)\n",
    "\n",
    "# M x M\n",
    "A_real = np.array([\n",
    "    [0.6, 0.3, 0.05, 0.05],\n",
    "    [0.2, 0.7, 0.05, 0.05],\n",
    "    [0.1, 0.1, 0.7, 0.1],\n",
    "    [0.05, 0.05, 0.2, 0.7]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "# M x N\n",
    "mus_real = np.array([\n",
    "    [20, 5],    # Warm & Rain\n",
    "    [22, 0],    # Warm & Dry\n",
    "    [5, 7],     # Cold & Rain\n",
    "    [3, 0]      # Cold & Dry\n",
    "], dtype=np.float32)\n",
    "\n",
    "# M x N x N\n",
    "Sigma_real = np.array([\n",
    "    [[3, 0], [0, 2]],\n",
    "    [[2, 0], [0, 1]],\n",
    "    [[4, 0], [0, 3]],\n",
    "    [[3, 0], [0, 1]]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# M\n",
    "pi_real = np.full(4, 0.25)\n",
    "\n",
    "observations_total = np.zeros((T + T_test, 2))\n",
    "states_total = np.zeros(T + T_test)\n",
    "\n",
    "states_total[0] = int(np.random.choice(4))\n",
    "print(states_total[0])\n",
    "observations_total[0, :] = np.random.multivariate_normal(mus_real[states_total[0], :], Sigma_real[states_total[0], :, :])\n",
    "\n",
    "for t in range(1, T + T_test):\n",
    "    # sample next state according to transition probabilities from A_real\n",
    "    states_total[t] = int(np.random.choice(4, p=A_real[states_total[t-1], :]))\n",
    "    observations_total[t] = np.random.multivariate_normal(mus_real[states_total[t], :], Sigma_real[states_total[t], :, :])\n",
    "\n",
    "# split data in training and testing set\n",
    "observations_train = observations_total[:T, :]\n",
    "observations_test = observations_total[T:, :]\n",
    "states_train = states_total[:T]\n",
    "states_test = states_total[T:]\n",
    "\n",
    "print('states:'); print(states_total)\n",
    "print()\n",
    "print('observations:'); print(observations_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e38be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Local\\Temp\\ipykernel_10868\\715241062.py:36: RuntimeWarning: invalid value encountered in log\n",
      "  logpi = np.log(pi)\n",
      "C:\\Users\\tobia\\AppData\\Local\\Temp\\ipykernel_10868\\4199790681.py:14: RuntimeWarning: invalid value encountered in reduce\n",
      "  logalpha[t, j] = np.logaddexp.reduce(logterms) + logb[t, j]\n",
      "C:\\Users\\tobia\\AppData\\Local\\Temp\\ipykernel_10868\\4199790681.py:32: RuntimeWarning: invalid value encountered in reduce\n",
      "  loggamma[t, i] = logalpha[t, i] + logbeta[t, i] - np.logaddexp.reduce([\n",
      "C:\\Users\\tobia\\AppData\\Local\\Temp\\ipykernel_10868\\4199790681.py:51: RuntimeWarning: invalid value encountered in reduce\n",
      "  - np.logaddexp.reduce(logterms)\n",
      "C:\\Users\\tobia\\AppData\\Local\\Temp\\ipykernel_10868\\4199790681.py:60: RuntimeWarning: invalid value encountered in reduce\n",
      "  np.logaddexp.reduce([logxi[t, i, j] for t in range(T - 1)])\n",
      "C:\\Users\\tobia\\AppData\\Local\\Temp\\ipykernel_10868\\4199790681.py:61: RuntimeWarning: invalid value encountered in reduce\n",
      "  - np.logaddexp.reduce([loggamma[t, i] for t in range(T - 1)])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     65\u001b[39m loga = compute_a(loggamma, logxi, T)\n\u001b[32m     66\u001b[39m mus, Sigmas = compute_mus_sigmas(loggamma, observations, T)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m logb = \u001b[43mcompute_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSigmas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m logpi = loggamma[\u001b[32m0\u001b[39m, :]\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# storing metrics for each iteration\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mcompute_b\u001b[39m\u001b[34m(mus, Sigmas, dataset, T)\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m4\u001b[39m):\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m np.isnan(dataset[t, :]).any() \u001b[38;5;129;01mor\u001b[39;00m np.isnan(mus[k, :]).any() \u001b[38;5;129;01mor\u001b[39;00m np.isnan(Sigmas[k, :, :]).any():\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n\u001b[32m     80\u001b[39m         logb[t, k] = multivariate_normal.logpdf(dataset[t, :], mus[k, :], Sigmas[k, :, :])\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logb\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "ll_tol = 1.0\n",
    "n_attempts = 100\n",
    "max_attempt_per_iter = 10\n",
    "\n",
    "# these will store data from every run\n",
    "# structure: list of lists (inner list = data per iteration)\n",
    "ll_list = []\n",
    "diff_norms_A = []\n",
    "diff_norms_B = []\n",
    "diff_norms_pi = []\n",
    "\n",
    "# these will store data from every run\n",
    "# stucture: list of floats (one per attempt, not one per iteration)\n",
    "pct_states_matched_viterbi = []\n",
    "frac_correct_states_predicted_list = []\n",
    "frac_correct_obs_predicted_list = []\n",
    "\n",
    "# initialize parameters for estimation (pretend we don't know the true values)\n",
    "A = A_real.copy()\n",
    "pi = pi_real.copy()\n",
    "\n",
    "for attempt in range(n_attempts):\n",
    "\n",
    "    prev_ll = 0\n",
    "    log_ll = 1.0\n",
    "    \n",
    "    # add noise to A\n",
    "    A += np.random.normal(0, A_std_noise, (4, 4))\n",
    "    A = np.clip(A, a_min=0.05, a_max=0.95)\n",
    "    A_rowsum = A.sum(axis=1)\n",
    "    for i in range(4):\n",
    "        A[:, i] /= A_rowsum\n",
    "    loga = np.log(A)\n",
    "\n",
    "    # add noise to pi\n",
    "    pi += np.random.normal(0, pi_std_noise, 4)\n",
    "    pi = np.clip(pi, a_min=0.05, a_max=0.95)\n",
    "    pi /= pi.sum()\n",
    "    logpi = np.log(pi)\n",
    "\n",
    "    # add noise to mus\n",
    "    mus = np.empty(mus_real.shape)\n",
    "    mus[:, 0] = np.clip(mus_real[:, 0] + np.random.normal(0, mus_std_noise[0], size=4), a_min=0, a_max=30)\n",
    "    mus[:, 1] = np.clip(mus_real[:, 1] + np.random.normal(0, mus_std_noise[1], size=4), a_min=0, a_max=30)\n",
    "    # add noise to Sigma\n",
    "    Sigma = Sigma_real.copy()\n",
    "    for m in range(4):\n",
    "        Sigma[m, :, :] = np.clip(Sigma_real[m, :, :] + Sigmas_std_noise * np.eye(2), a_min=0, a_max=10)\n",
    "\n",
    "    # compute logB\n",
    "    logb = compute_b(mus, Sigma, observations_train, T)\n",
    "\n",
    "    # store some metrics for each iterion step\n",
    "    this_diff_norms_A = []\n",
    "    this_diff_norms_B = []\n",
    "    this_diff_norms_pi = []\n",
    "    this_attempt_ll_list = []\n",
    "\n",
    "    this_attempt_count = 0\n",
    "\n",
    "    while abs(prev_ll - log_ll) > ll_tol and this_attempt_count < max_attempt_per_iter:\n",
    "        print('dist:', prev_ll - log_ll)\n",
    "        prev_ll = log_ll\n",
    "\n",
    "        # E step\n",
    "        logalpha = forward(loga, logb, T, logpi, observations_train)\n",
    "        logbeta = backward(loga, logb, T)\n",
    "        loggamma = compute_gamma(logalpha, logbeta, T)\n",
    "        logxi = compute_xi(logalpha, logbeta, loga, logb, observations_train, T)\n",
    "\n",
    "        # M step\n",
    "        loga = compute_a(loggamma, logxi, T)\n",
    "        mus, Sigmas = compute_mus_sigmas(loggamma, observations_train, T)\n",
    "        logb = compute_b(mus, Sigmas, observations_train, T)\n",
    "        logpi = loggamma[0, :]\n",
    "\n",
    "        # storing metrics for each iteration\n",
    "        log_ll = np.logaddexp.reduce(logalpha[-1, :])\n",
    "        this_attempt_ll_list.append(log_ll)\n",
    "\n",
    "        this_attempt_count += 1\n",
    "\n",
    "    A = np.exp(loga)\n",
    "    A /= A.sum(axis=1, keepdims=True)\n",
    "    B = np.exp(logb)\n",
    "    B /= B.sum(axis=1, keepdims=True)\n",
    "    pi = np.exp(logpi)\n",
    "    pi /= pi.sum()\n",
    "\n",
    "    A = np.clip(A, 1e-10, 1)\n",
    "    B = np.clip(B, 1e-10, 1)\n",
    "\n",
    "    # store data from this attempt\n",
    "    ll_list.append(this_attempt_ll_list)\n",
    "    diff_norms_A.append(this_diff_norms_A)\n",
    "    diff_norms_B.append(this_diff_norms_B)\n",
    "    diff_norms_pi.append(this_diff_norms_pi)\n",
    "\n",
    "    # test model on training data\n",
    "\n",
    "    # first, use the Hungarian method to match the most likely B_real orientation, given the\n",
    "    # orientation of B\n",
    "    perm = match_states_by_B(B, B_real)\n",
    "    pi_p, A_p, B_p = permute_model(pi, A, B, perm)\n",
    "\n",
    "    logpi_p = np.log(pi_p)\n",
    "    loga_p = np.log(A_p)\n",
    "    logb_p = np.log(B_p)\n",
    "\n",
    "    # assuming we have access to the future observations, we will test the model using the viterbi path\n",
    "    # algorithm, to see how well it can predict the hidden state variables, given these observations\n",
    "    predicted_states_vit, p = viterbi(logpi_p, logb_p, loga_p, observations_test, T_test)\n",
    "    pct_matched = 1 - np.count_nonzero(np.logical_xor(predicted_states_vit, states_test)) / len(predicted_states_vit)\n",
    "    pct_states_matched_viterbi.append(pct_matched)\n",
    "\n",
    "    # assuming we are at time T and we wanted to predict the future states and observations up untill\n",
    "    # t = T_test, we will try to predict these values and see how well the model generalizes\n",
    "    predicted_obs = np.empty((T_test, 2))\n",
    "    predicted_states = np.empty(T_test)\n",
    "\n",
    "    states_p_dist = np.exp(loggamma[-1, :])\n",
    "    predicted_states[0] = np.argmax(states_p_dist)\n",
    "    predicted_obs[0] = np.argmax(B_p[int(predicted_states[0]), :])\n",
    "\n",
    "    # generate prediction states and observations using A and B\n",
    "    for k in range(1, T_test):\n",
    "\n",
    "        # prediction of states\n",
    "        states_p_dist = states_p_dist @ A_p\n",
    "        states_p_dist /= states_p_dist.sum()\n",
    "        predicted_states[k] = np.argmax(states_p_dist)\n",
    "        # determine the predicted observation by taking the most likely observation, given the\n",
    "        # predicted state value\n",
    "        predicted_obs[k, :] = pi @ mus\n",
    "\n",
    "    # save the fraction of correctly predicted states and observations\n",
    "    frac_correct_states_predicted = 1 - np.count_nonzero(np.logical_xor(states_test, predicted_states)) / len(predicted_states)\n",
    "    frac_correct_obs_predicted = 1 - np.count_nonzero(np.logical_xor(observations_test, predicted_obs)) / len(predicted_obs)\n",
    "\n",
    "    frac_correct_states_predicted_list.append(frac_correct_states_predicted)\n",
    "    frac_correct_obs_predicted_list.append(frac_correct_obs_predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37513499",
   "metadata": {},
   "source": [
    "Plot the log-likelyhood values of each attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dacaf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(n_attempts):\n",
    "    ax.plot(list(range(len(ll_list[i]))), ll_list[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45581bde",
   "metadata": {},
   "source": [
    "Plot the Frobenius norm indicating the error between the real value and the estimated value for the matrices $A$, $B$ and $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "\n",
    "for i in range(n_attempts):\n",
    "    ax1.plot(diff_norms_A[i])\n",
    "    ax2.plot(diff_norms_B[i])\n",
    "    ax3.plot(diff_norms_pi[i])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9542565",
   "metadata": {},
   "source": [
    "Make a bar chart out of the fraction of correctly predicted states, given the true observations from $t=T_{\\text{test}}$ onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc06c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.linspace(0, 1, n_attempts), pct_states_matched_viterbi, width=1/(1 + n_attempts))\n",
    "plt.show()\n",
    "\n",
    "print('maximum pct correct:', np.max(pct_states_matched_viterbi), 'from attempt', np.argmax(pct_states_matched_viterbi) + 1)\n",
    "print('minimum pct correct:', np.min(pct_states_matched_viterbi), 'from attempt', np.argmin(pct_states_matched_viterbi) + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
